# -*- coding: utf-8 -*-
"""EMNIST_PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oANVuuhxxh9LdVFmdgG0AffBNBbjMsXx
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transfroms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt


batch_size = 256
learning_rate = 0.000725
num_classes = 26
num_epochs = 10

emnist_train = torchvision.datasets.EMNIST(root = './data/EMNIST', split = 'letters', train = True, download = True, transform = transfroms.Compose([transfroms.ToTensor()]))
emnist_test = torchvision.datasets.EMNIST(root = './data/EMNIST', split = 'letters', train = False, download = True, transform = transfroms.Compose([transfroms.ToTensor()]))
train_loader = torch.utils.data.DataLoader(emnist_train, batch_size=batch_size)
test_loader = torch.utils.data.DataLoader(emnist_test, batch_size=batch_size)
print(len(emnist_train)) #train 데이터의 개수
print(len(emnist_test)) #test 데이터의 개수

import matplotlib.pyplot as plt
plt.imshow(emnist_train[0][0].reshape(28,28))
print(emnist_train[0][1]) #첫번째 데이터의 알파벳
#train 데이터셋에서 첫 번째 데이터 시각화

plt.imshow(emnist_test[0][0].reshape(28,28))
print(emnist_test[0][1]) #첫번째 데이터의 알파벳
#test 데이터셋에서 첫 번째 데이터 시각화

class CNN(nn.Module):
    def __init__(self):
        super(CNN,self).__init__()
        self.layer = nn.Sequential(
            nn.Conv2d(in_channels=1,out_channels=10,kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,stride=2),
            nn.Conv2d(in_channels=10,out_channels=100,kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2,stride=2)
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(100*7*7,1000),
            nn.ReLU(),
            nn.Linear(1000,26)
        )       
        self.dropout = nn.Dropout()
    def forward(self, x):
        out = self.layer(x)
        out = out.reshape(out.size(0), -1)
        out = self.dropout(out)
        out = self.fc_layer(out)
        return out

model = CNN()
loss_func = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

loss_arr =[]
for i in range(num_epochs):
    for j, [images, label] in enumerate(train_loader):
        out = model(images)
        loss = loss_func(out, label-1)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if j % 200 == 0:
            print(loss)
            loss_arr.append(loss.cpu().detach().numpy())

plt.plot(loss_arr, 'r')
plt.title('Loss graph')
plt.show()

correct = 0
total = 0

with torch.no_grad():
    for i, [images,label] in enumerate(test_loader):
        x = model(images)
        _,output_index = torch.max(x.data,1)
        total += len(label)
        correct += (output_index == label-1).sum().float()

    print("Accuracy of Test Data: {}%".format(100*correct/total))

x = [92.293, 92.5433, 92.5625, 92.3990, 92.4375] #epoch5
plt.plot(x, 'ro', label='epoch 5')
x2 = [92.8255, 92.9471, 93.2115, 92.9904, 92.9471] #epoch10
values = ['0.0005', '0.000675', '0.000725', '0.00075','0.000825'] 
plt.plot(x2, 'bo', label='epoch 10')
plt.xlabel('learning rate')
plt.ylabel('accuracy')
plt.legend()
plt.xticks([0,1,2,3,4],values)
plt.show()

"""**201602391 박현우**"""